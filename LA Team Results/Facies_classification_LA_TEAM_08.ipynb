{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facies classification using Machine Learning #\n",
    "## LA Team Submission 6 ## \n",
    "### _[Lukas Mosser](https://at.linkedin.com/in/lukas-mosser-9948b32b/en), [Alfredo De la Fuente](https://pe.linkedin.com/in/alfredodelafuenteb)_ ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this approach for solving the facies classfication problem ( https://github.com/seg/2016-ml-contest. ) we will explore the following statregies:\n",
    "- Features Exploration: based on [Paolo Bestagini's work](https://github.com/seg/2016-ml-contest/blob/master/ispl/facies_classification_try02.ipynb), we will consider imputation, normalization and augmentation routines for the initial features.\n",
    "- Model tuning: we use TPOT to come up with a good enough pipeline, and then tune the hyperparameters of the model obtained using HYPEROPT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): pandas in /home/alfredo/anaconda2/lib/python2.7/site-packages\n",
      "Requirement already satisfied (use --upgrade to upgrade): python-dateutil in /home/alfredo/anaconda2/lib/python2.7/site-packages (from pandas)\n",
      "Requirement already satisfied (use --upgrade to upgrade): pytz>=2011k in /home/alfredo/anaconda2/lib/python2.7/site-packages (from pandas)\n",
      "Requirement already satisfied (use --upgrade to upgrade): numpy>=1.7.0 in /home/alfredo/anaconda2/lib/python2.7/site-packages (from pandas)\n",
      "Requirement already satisfied (use --upgrade to upgrade): six>=1.5 in /home/alfredo/anaconda2/lib/python2.7/site-packages (from python-dateutil->pandas)\n",
      "Requirement already satisfied (use --upgrade to upgrade): scikit-learn in /home/alfredo/anaconda2/lib/python2.7/site-packages\n",
      "Requirement already satisfied (use --upgrade to upgrade): tpot in /home/alfredo/anaconda2/lib/python2.7/site-packages\n",
      "Requirement already satisfied (use --upgrade to upgrade): deap in /home/alfredo/anaconda2/lib/python2.7/site-packages (from tpot)\n",
      "Requirement already satisfied (use --upgrade to upgrade): numpy in /home/alfredo/anaconda2/lib/python2.7/site-packages (from tpot)\n",
      "Requirement already satisfied (use --upgrade to upgrade): update-checker in /home/alfredo/anaconda2/lib/python2.7/site-packages (from tpot)\n",
      "Requirement already satisfied (use --upgrade to upgrade): scipy in /home/alfredo/anaconda2/lib/python2.7/site-packages (from tpot)\n",
      "Requirement already satisfied (use --upgrade to upgrade): tqdm in /home/alfredo/anaconda2/lib/python2.7/site-packages (from tpot)\n",
      "Requirement already satisfied (use --upgrade to upgrade): scikit-learn in /home/alfredo/anaconda2/lib/python2.7/site-packages (from tpot)\n",
      "Requirement already satisfied (use --upgrade to upgrade): requests>=2.3.0 in /home/alfredo/anaconda2/lib/python2.7/site-packages (from update-checker->tpot)\n",
      "Requirement already satisfied (use --upgrade to upgrade): hyperopt in /home/alfredo/anaconda2/lib/python2.7/site-packages\n",
      "Requirement already satisfied (use --upgrade to upgrade): numpy in /home/alfredo/anaconda2/lib/python2.7/site-packages (from hyperopt)\n",
      "Requirement already satisfied (use --upgrade to upgrade): networkx in /home/alfredo/anaconda2/lib/python2.7/site-packages (from hyperopt)\n",
      "Requirement already satisfied (use --upgrade to upgrade): nose in /home/alfredo/anaconda2/lib/python2.7/site-packages (from hyperopt)\n",
      "Requirement already satisfied (use --upgrade to upgrade): future in /home/alfredo/anaconda2/lib/python2.7/site-packages (from hyperopt)\n",
      "Requirement already satisfied (use --upgrade to upgrade): scipy in /home/alfredo/anaconda2/lib/python2.7/site-packages (from hyperopt)\n",
      "Requirement already satisfied (use --upgrade to upgrade): six in /home/alfredo/anaconda2/lib/python2.7/site-packages (from hyperopt)\n",
      "Requirement already satisfied (use --upgrade to upgrade): pymongo in /home/alfredo/anaconda2/lib/python2.7/site-packages (from hyperopt)\n",
      "Requirement already satisfied (use --upgrade to upgrade): decorator>=3.4.0 in /home/alfredo/anaconda2/lib/python2.7/site-packages (from networkx->hyperopt)\n",
      "Requirement already satisfied (use --upgrade to upgrade): xgboost in /home/alfredo/anaconda2/lib/python2.7/site-packages\n",
      "Requirement already satisfied (use --upgrade to upgrade): numpy in /home/alfredo/anaconda2/lib/python2.7/site-packages (from xgboost)\n",
      "Requirement already satisfied (use --upgrade to upgrade): scipy in /home/alfredo/anaconda2/lib/python2.7/site-packages (from xgboost)\n",
      "Requirement already satisfied (use --upgrade to upgrade): scikit-learn in /home/alfredo/anaconda2/lib/python2.7/site-packages (from xgboost)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 8.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "You are using pip version 8.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "You are using pip version 8.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "You are using pip version 8.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "You are using pip version 8.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "pip install pandas\n",
    "pip install scikit-learn\n",
    "pip install tpot\n",
    "pip install hyperopt\n",
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold , StratifiedKFold\n",
    "from classification_utilities import display_cm, display_adj_cm\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import LeavePGroupsOut\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from scipy.signal import medfilt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing \n",
    "\n",
    "We procceed to run [Paolo Bestagini's routine](https://github.com/seg/2016-ml-contest/blob/master/ispl/facies_classification_try02.ipynb) to include a small window of values to acount for the spatial component in the log analysis, as well as the gradient information with respect to depth. This will be our prepared training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load Data\n",
    "data = pd.read_csv('../facies_vectors.csv')\n",
    "\n",
    "# Parameters\n",
    "feature_names = ['GR', 'ILD_log10', 'DeltaPHI', 'PHIND', 'PE', 'NM_M', 'RELPOS']\n",
    "facies_names = ['SS', 'CSiS', 'FSiS', 'SiSh', 'MS', 'WS', 'D', 'PS', 'BS']\n",
    "facies_colors = ['#F4D03F', '#F5B041','#DC7633','#6E2C00', '#1B4F72','#2E86C1', '#AED6F1', '#A569BD', '#196F3D']\n",
    "\n",
    "# Store features and labels\n",
    "X = data[feature_names].values \n",
    "y = data['Facies'].values \n",
    "\n",
    "# Store well labels and depths\n",
    "well = data['Well Name'].values\n",
    "depth = data['Depth'].values\n",
    "\n",
    "# Fill 'PE' missing values with mean\n",
    "imp = preprocessing.Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "imp.fit(X)\n",
    "X = imp.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature windows concatenation function\n",
    "def augment_features_window(X, N_neig):\n",
    "    \n",
    "    # Parameters\n",
    "    N_row = X.shape[0]\n",
    "    N_feat = X.shape[1]\n",
    "\n",
    "    # Zero padding\n",
    "    X = np.vstack((np.zeros((N_neig, N_feat)), X, (np.zeros((N_neig, N_feat)))))\n",
    "\n",
    "    # Loop over windows\n",
    "    X_aug = np.zeros((N_row, N_feat*(2*N_neig+1)))\n",
    "    for r in np.arange(N_row)+N_neig:\n",
    "        this_row = []\n",
    "        for c in np.arange(-N_neig,N_neig+1):\n",
    "            this_row = np.hstack((this_row, X[r+c]))\n",
    "        X_aug[r-N_neig] = this_row\n",
    "\n",
    "    return X_aug\n",
    "\n",
    "\n",
    "# Feature gradient computation function\n",
    "def augment_features_gradient(X, depth):\n",
    "    \n",
    "    # Compute features gradient\n",
    "    d_diff = np.diff(depth).reshape((-1, 1))\n",
    "    d_diff[d_diff==0] = 0.001\n",
    "    X_diff = np.diff(X, axis=0)\n",
    "    X_grad = X_diff / d_diff\n",
    "        \n",
    "    # Compensate for last missing value\n",
    "    X_grad = np.concatenate((X_grad, np.zeros((1, X_grad.shape[1]))))\n",
    "    \n",
    "    return X_grad\n",
    "\n",
    "\n",
    "# Feature augmentation function\n",
    "def augment_features(X, well, depth, N_neig=1):\n",
    "    \n",
    "    # Augment features\n",
    "    X_aug = np.zeros((X.shape[0], X.shape[1]*(N_neig*2+2)))\n",
    "    for w in np.unique(well):\n",
    "        w_idx = np.where(well == w)[0]\n",
    "        X_aug_win = augment_features_window(X[w_idx, :], N_neig)\n",
    "        X_aug_grad = augment_features_gradient(X[w_idx, :], depth[w_idx])\n",
    "        X_aug[w_idx, :] = np.concatenate((X_aug_win, X_aug_grad), axis=1)\n",
    "    \n",
    "    # Find padded rows\n",
    "    padded_rows = np.unique(np.where(X_aug[:, 0:7] == np.zeros((1, 7)))[0])\n",
    "    \n",
    "    return X_aug, padded_rows\n",
    "\n",
    "X_aug, padded_rows = augment_features(X, well, depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize model selection methods\n",
    "lpgo = LeavePGroupsOut(2)\n",
    "\n",
    "# Generate splits\n",
    "split_list = []\n",
    "for train, val in lpgo.split(X, y, groups=data['Well Name']):\n",
    "    hist_tr = np.histogram(y[train], bins=np.arange(len(facies_names)+1)+.5)\n",
    "    hist_val = np.histogram(y[val], bins=np.arange(len(facies_names)+1)+.5)\n",
    "    if np.all(hist_tr[0] != 0) & np.all(hist_val[0] != 0):\n",
    "        split_list.append({'train':train, 'val':val})\n",
    "    \n",
    "        \n",
    "def preprocess():\n",
    "    \n",
    "    # Preprocess data to use in model\n",
    "    X_train_aux = []\n",
    "    X_test_aux = []\n",
    "    y_train_aux = []\n",
    "    y_test_aux = []\n",
    "    \n",
    "    # For each data split\n",
    "    for split in split_list:\n",
    "        # Remove padded rows\n",
    "        split_train_no_pad = np.setdiff1d(split['train'], padded_rows)\n",
    "\n",
    "        # Select training and validation data from current split\n",
    "        X_tr = X_aug[split_train_no_pad, :]\n",
    "        X_v = X_aug[split['val'], :]\n",
    "        y_tr = y[split_train_no_pad]\n",
    "        y_v = y[split['val']]\n",
    "\n",
    "        # Select well labels for validation data\n",
    "        well_v = well[split['val']]\n",
    "\n",
    "        # Feature normalization\n",
    "        scaler = preprocessing.RobustScaler(quantile_range=(25.0, 75.0)).fit(X_tr)\n",
    "        X_tr = scaler.transform(X_tr)\n",
    "        X_v = scaler.transform(X_v)\n",
    "\n",
    "        X_train_aux.append( X_tr )\n",
    "        X_test_aux.append( X_v )\n",
    "        y_train_aux.append( y_tr )\n",
    "        y_test_aux.append (  y_v )\n",
    "\n",
    "        X_train = np.concatenate( X_train_aux )\n",
    "        X_test = np.concatenate ( X_test_aux )\n",
    "        y_train = np.concatenate ( y_train_aux )\n",
    "        y_test = np.concatenate ( y_test_aux )\n",
    "    \n",
    "    return X_train , X_test , y_train , y_test \n",
    "\n",
    "X_train, X_test, y_train, y_test = preprocess()\n",
    "y_train = y_train - 1 \n",
    "y_test = y_test - 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis\n",
    "\n",
    "In this section we will run a Cross Validation routine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost.sklearn import  XGBClassifier\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SEED = 314159265\n",
    "VALID_SIZE = 0.2\n",
    "TARGET = 'outcome'\n",
    "\n",
    "# Scoring and optimization functions\n",
    "\n",
    "def score(params):\n",
    "    print(\"Training with params: \")\n",
    "    print(params)\n",
    "    #clf = xgb.XGBClassifier(**params) \n",
    "    #clf.fit(X_train, y_train)\n",
    "    #y_predictions = clf.predict(X_test)\n",
    "    num_round = int(params['n_estimators'])\n",
    "    del params['n_estimators']\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dvalid = xgb.DMatrix(X_test, label=y_test)\n",
    "    watchlist = [(dvalid, 'eval'), (dtrain, 'train')]\n",
    "    gbm_model = xgb.train(params, dtrain, num_round,\n",
    "                          evals=watchlist,\n",
    "                          verbose_eval=True)\n",
    "    y_predictions = gbm_model.predict(dvalid,\n",
    "                                    ntree_limit=gbm_model.best_iteration + 1)\n",
    "    \n",
    "    score = f1_score (y_test, y_predictions , average ='micro')\n",
    "    print(\"\\tScore {0}\\n\\n\".format(score))\n",
    "    loss = 1 - score\n",
    "    return {'loss': loss, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "def optimize(random_state=SEED):\n",
    "    space = {\n",
    "        'n_estimators': hp.quniform('n_estimators', 100, 150, 1),\n",
    "        'eta': hp.quniform('eta', 0.025, 0.5, 0.025),\n",
    "        'max_depth':  hp.choice('max_depth', np.arange(1, 14, dtype=int)),\n",
    "        'min_child_weight': hp.quniform('min_child_weight', 1, 6, 1),\n",
    "        'subsample': hp.quniform('subsample', 0.5, 1, 0.05),\n",
    "        'gamma': hp.quniform('gamma', 0.5, 1, 0.05),\n",
    "        'colsample_bytree': hp.quniform('colsample_bytree', 0.5, 1, 0.05),\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'objective': 'multi:softmax',\n",
    "        'nthread': 4,\n",
    "        'booster': 'gbtree',\n",
    "        'tree_method': 'exact',\n",
    "        'silent': 1,\n",
    "        'num_class' : 9,\n",
    "        'seed': random_state\n",
    "    }\n",
    "    # Use the fmin function from Hyperopt to find the best hyperparameters\n",
    "    best = fmin(score, space, algo=tpe.suggest,  max_evals=5)\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with params: \n",
      "{'colsample_bytree': 0.8500000000000001, 'silent': 1, 'eval_metric': 'mlogloss', 'nthread': 4, 'min_child_weight': 4.0, 'n_estimators': 148.0, 'subsample': 0.9, 'eta': 0.15000000000000002, 'objective': 'multi:softmax', 'num_class': 9, 'tree_method': 'exact', 'seed': 314159265, 'max_depth': 9, 'gamma': 0.75, 'booster': 'gbtree'}\n",
      "[0]\teval-mlogloss:1.77004\ttrain-mlogloss:1.75484\n",
      "[1]\teval-mlogloss:1.51325\ttrain-mlogloss:1.48608\n",
      "[2]\teval-mlogloss:1.31655\ttrain-mlogloss:1.28232\n",
      "[3]\teval-mlogloss:1.16507\ttrain-mlogloss:1.12506\n",
      "[4]\teval-mlogloss:1.0393\ttrain-mlogloss:0.995635\n",
      "[5]\teval-mlogloss:0.935651\ttrain-mlogloss:0.889398\n",
      "[6]\teval-mlogloss:0.847376\ttrain-mlogloss:0.798796\n",
      "[7]\teval-mlogloss:0.770582\ttrain-mlogloss:0.720501\n",
      "[8]\teval-mlogloss:0.703384\ttrain-mlogloss:0.652322\n",
      "[9]\teval-mlogloss:0.64605\ttrain-mlogloss:0.594215\n",
      "[10]\teval-mlogloss:0.594241\ttrain-mlogloss:0.541877\n",
      "[11]\teval-mlogloss:0.548435\ttrain-mlogloss:0.49588\n",
      "[12]\teval-mlogloss:0.507176\ttrain-mlogloss:0.454374\n",
      "[13]\teval-mlogloss:0.472222\ttrain-mlogloss:0.419359\n",
      "[14]\teval-mlogloss:0.440209\ttrain-mlogloss:0.38774\n",
      "[15]\teval-mlogloss:0.410642\ttrain-mlogloss:0.358263\n",
      "[16]\teval-mlogloss:0.384208\ttrain-mlogloss:0.332742\n",
      "[17]\teval-mlogloss:0.360306\ttrain-mlogloss:0.308959\n",
      "[18]\teval-mlogloss:0.339444\ttrain-mlogloss:0.288337\n",
      "[19]\teval-mlogloss:0.320635\ttrain-mlogloss:0.27024\n",
      "[20]\teval-mlogloss:0.30304\ttrain-mlogloss:0.253228\n",
      "[21]\teval-mlogloss:0.287396\ttrain-mlogloss:0.238097\n",
      "[22]\teval-mlogloss:0.274009\ttrain-mlogloss:0.225423\n",
      "[23]\teval-mlogloss:0.260853\ttrain-mlogloss:0.212888\n",
      "[24]\teval-mlogloss:0.249767\ttrain-mlogloss:0.202529\n",
      "[25]\teval-mlogloss:0.238788\ttrain-mlogloss:0.192004\n",
      "[26]\teval-mlogloss:0.22815\ttrain-mlogloss:0.181666\n",
      "[27]\teval-mlogloss:0.219896\ttrain-mlogloss:0.173735\n",
      "[28]\teval-mlogloss:0.210887\ttrain-mlogloss:0.165314\n",
      "[29]\teval-mlogloss:0.203215\ttrain-mlogloss:0.15805\n",
      "[30]\teval-mlogloss:0.195757\ttrain-mlogloss:0.151125\n",
      "[31]\teval-mlogloss:0.187306\ttrain-mlogloss:0.143409\n",
      "[32]\teval-mlogloss:0.179986\ttrain-mlogloss:0.136504\n",
      "[33]\teval-mlogloss:0.173018\ttrain-mlogloss:0.130126\n",
      "[34]\teval-mlogloss:0.166742\ttrain-mlogloss:0.124243\n",
      "[35]\teval-mlogloss:0.160619\ttrain-mlogloss:0.118565\n",
      "[36]\teval-mlogloss:0.154906\ttrain-mlogloss:0.1134\n",
      "[37]\teval-mlogloss:0.149827\ttrain-mlogloss:0.108622\n",
      "[38]\teval-mlogloss:0.144828\ttrain-mlogloss:0.103783\n",
      "[39]\teval-mlogloss:0.139478\ttrain-mlogloss:0.09876\n",
      "[40]\teval-mlogloss:0.134523\ttrain-mlogloss:0.094516\n",
      "[41]\teval-mlogloss:0.13065\ttrain-mlogloss:0.09091\n",
      "[42]\teval-mlogloss:0.126109\ttrain-mlogloss:0.086819\n",
      "[43]\teval-mlogloss:0.121928\ttrain-mlogloss:0.082895\n",
      "[44]\teval-mlogloss:0.118028\ttrain-mlogloss:0.079453\n",
      "[45]\teval-mlogloss:0.11356\ttrain-mlogloss:0.075485\n",
      "[46]\teval-mlogloss:0.110264\ttrain-mlogloss:0.072469\n",
      "[47]\teval-mlogloss:0.107411\ttrain-mlogloss:0.069873\n",
      "[48]\teval-mlogloss:0.103915\ttrain-mlogloss:0.066762\n",
      "[49]\teval-mlogloss:0.100752\ttrain-mlogloss:0.063897\n",
      "[50]\teval-mlogloss:0.097958\ttrain-mlogloss:0.06156\n",
      "[51]\teval-mlogloss:0.095155\ttrain-mlogloss:0.059015\n",
      "[52]\teval-mlogloss:0.092131\ttrain-mlogloss:0.056393\n",
      "[53]\teval-mlogloss:0.089288\ttrain-mlogloss:0.053926\n",
      "[54]\teval-mlogloss:0.08711\ttrain-mlogloss:0.052081\n",
      "[55]\teval-mlogloss:0.085152\ttrain-mlogloss:0.050433\n",
      "[56]\teval-mlogloss:0.082962\ttrain-mlogloss:0.048555\n",
      "[57]\teval-mlogloss:0.080881\ttrain-mlogloss:0.046698\n",
      "[58]\teval-mlogloss:0.078692\ttrain-mlogloss:0.044856\n",
      "[59]\teval-mlogloss:0.076667\ttrain-mlogloss:0.043168\n",
      "[60]\teval-mlogloss:0.074782\ttrain-mlogloss:0.041527\n",
      "[61]\teval-mlogloss:0.073075\ttrain-mlogloss:0.040189\n",
      "[62]\teval-mlogloss:0.071012\ttrain-mlogloss:0.038506\n",
      "[63]\teval-mlogloss:0.069498\ttrain-mlogloss:0.037268\n",
      "[64]\teval-mlogloss:0.067669\ttrain-mlogloss:0.035846\n",
      "[65]\teval-mlogloss:0.066142\ttrain-mlogloss:0.03463\n",
      "[66]\teval-mlogloss:0.06487\ttrain-mlogloss:0.033471\n",
      "[67]\teval-mlogloss:0.063373\ttrain-mlogloss:0.0323\n",
      "[68]\teval-mlogloss:0.061974\ttrain-mlogloss:0.031185\n",
      "[69]\teval-mlogloss:0.060628\ttrain-mlogloss:0.030123\n",
      "[70]\teval-mlogloss:0.059831\ttrain-mlogloss:0.029479\n",
      "[71]\teval-mlogloss:0.058782\ttrain-mlogloss:0.028643\n",
      "[72]\teval-mlogloss:0.057335\ttrain-mlogloss:0.027521\n",
      "[73]\teval-mlogloss:0.055983\ttrain-mlogloss:0.026461\n",
      "[74]\teval-mlogloss:0.054857\ttrain-mlogloss:0.025644\n",
      "[75]\teval-mlogloss:0.053828\ttrain-mlogloss:0.024907\n",
      "[76]\teval-mlogloss:0.052583\ttrain-mlogloss:0.023964\n",
      "[77]\teval-mlogloss:0.051601\ttrain-mlogloss:0.023283\n",
      "[78]\teval-mlogloss:0.050518\ttrain-mlogloss:0.022443\n",
      "[79]\teval-mlogloss:0.049933\ttrain-mlogloss:0.021966\n",
      "[80]\teval-mlogloss:0.049042\ttrain-mlogloss:0.021303\n",
      "[81]\teval-mlogloss:0.048475\ttrain-mlogloss:0.020866\n",
      "[82]\teval-mlogloss:0.047539\ttrain-mlogloss:0.020203\n",
      "[83]\teval-mlogloss:0.046641\ttrain-mlogloss:0.019567\n",
      "[84]\teval-mlogloss:0.04593\ttrain-mlogloss:0.019053\n",
      "[85]\teval-mlogloss:0.045267\ttrain-mlogloss:0.018558\n",
      "[86]\teval-mlogloss:0.044716\ttrain-mlogloss:0.018166\n",
      "[87]\teval-mlogloss:0.044155\ttrain-mlogloss:0.017757\n",
      "[88]\teval-mlogloss:0.043403\ttrain-mlogloss:0.01719\n",
      "[89]\teval-mlogloss:0.042785\ttrain-mlogloss:0.01672\n",
      "[90]\teval-mlogloss:0.04229\ttrain-mlogloss:0.016347\n",
      "[91]\teval-mlogloss:0.041787\ttrain-mlogloss:0.016025\n",
      "[92]\teval-mlogloss:0.041198\ttrain-mlogloss:0.015666\n",
      "[93]\teval-mlogloss:0.040804\ttrain-mlogloss:0.015406\n",
      "[94]\teval-mlogloss:0.040351\ttrain-mlogloss:0.015118\n",
      "[95]\teval-mlogloss:0.039917\ttrain-mlogloss:0.014818\n",
      "[96]\teval-mlogloss:0.039725\ttrain-mlogloss:0.014635\n",
      "[97]\teval-mlogloss:0.039294\ttrain-mlogloss:0.014343\n",
      "[98]\teval-mlogloss:0.039041\ttrain-mlogloss:0.014163\n",
      "[99]\teval-mlogloss:0.038654\ttrain-mlogloss:0.013873\n",
      "[100]\teval-mlogloss:0.038336\ttrain-mlogloss:0.013704\n",
      "[101]\teval-mlogloss:0.038138\ttrain-mlogloss:0.013552\n",
      "[102]\teval-mlogloss:0.037895\ttrain-mlogloss:0.01338\n",
      "[103]\teval-mlogloss:0.037624\ttrain-mlogloss:0.013187\n",
      "[104]\teval-mlogloss:0.037433\ttrain-mlogloss:0.013077\n",
      "[105]\teval-mlogloss:0.037225\ttrain-mlogloss:0.01294\n",
      "[106]\teval-mlogloss:0.036978\ttrain-mlogloss:0.012761\n",
      "[107]\teval-mlogloss:0.03666\ttrain-mlogloss:0.01254\n",
      "[108]\teval-mlogloss:0.036486\ttrain-mlogloss:0.012426\n",
      "[109]\teval-mlogloss:0.036242\ttrain-mlogloss:0.012227\n",
      "[110]\teval-mlogloss:0.036007\ttrain-mlogloss:0.012133\n",
      "[111]\teval-mlogloss:0.035735\ttrain-mlogloss:0.011947\n",
      "[112]\teval-mlogloss:0.035608\ttrain-mlogloss:0.011853\n",
      "[113]\teval-mlogloss:0.035451\ttrain-mlogloss:0.011744\n",
      "[114]\teval-mlogloss:0.035227\ttrain-mlogloss:0.011612\n",
      "[115]\teval-mlogloss:0.035013\ttrain-mlogloss:0.011487\n",
      "[116]\teval-mlogloss:0.034848\ttrain-mlogloss:0.01136\n",
      "[117]\teval-mlogloss:0.034715\ttrain-mlogloss:0.011269\n",
      "[118]\teval-mlogloss:0.034597\ttrain-mlogloss:0.011159\n",
      "[119]\teval-mlogloss:0.034431\ttrain-mlogloss:0.011081\n",
      "[120]\teval-mlogloss:0.03425\ttrain-mlogloss:0.011006\n",
      "[121]\teval-mlogloss:0.03401\ttrain-mlogloss:0.010901\n",
      "[122]\teval-mlogloss:0.033744\ttrain-mlogloss:0.01078\n",
      "[123]\teval-mlogloss:0.033637\ttrain-mlogloss:0.010712\n",
      "[124]\teval-mlogloss:0.033449\ttrain-mlogloss:0.010621\n",
      "[125]\teval-mlogloss:0.033386\ttrain-mlogloss:0.010585\n",
      "[126]\teval-mlogloss:0.03331\ttrain-mlogloss:0.010549\n",
      "[127]\teval-mlogloss:0.033202\ttrain-mlogloss:0.010483\n",
      "[128]\teval-mlogloss:0.033064\ttrain-mlogloss:0.010427\n",
      "[129]\teval-mlogloss:0.03295\ttrain-mlogloss:0.010363\n",
      "[130]\teval-mlogloss:0.032851\ttrain-mlogloss:0.010292\n",
      "[131]\teval-mlogloss:0.032744\ttrain-mlogloss:0.010225\n",
      "[132]\teval-mlogloss:0.03264\ttrain-mlogloss:0.010157\n",
      "[133]\teval-mlogloss:0.032553\ttrain-mlogloss:0.010103\n",
      "[134]\teval-mlogloss:0.032441\ttrain-mlogloss:0.010039\n",
      "[135]\teval-mlogloss:0.032316\ttrain-mlogloss:0.009974\n",
      "[136]\teval-mlogloss:0.032236\ttrain-mlogloss:0.009931\n",
      "[137]\teval-mlogloss:0.032182\ttrain-mlogloss:0.009897\n",
      "[138]\teval-mlogloss:0.032111\ttrain-mlogloss:0.009863\n",
      "[139]\teval-mlogloss:0.032051\ttrain-mlogloss:0.009826\n",
      "[140]\teval-mlogloss:0.031924\ttrain-mlogloss:0.009761\n",
      "[141]\teval-mlogloss:0.031845\ttrain-mlogloss:0.009709\n",
      "[142]\teval-mlogloss:0.031726\ttrain-mlogloss:0.009656\n",
      "[143]\teval-mlogloss:0.031667\ttrain-mlogloss:0.009621\n",
      "[144]\teval-mlogloss:0.031598\ttrain-mlogloss:0.009565\n",
      "[145]\teval-mlogloss:0.031484\ttrain-mlogloss:0.00949\n",
      "[146]\teval-mlogloss:0.031468\ttrain-mlogloss:0.009471\n",
      "[147]\teval-mlogloss:0.031395\ttrain-mlogloss:0.00945\n",
      "\tScore 0.994896053778\n",
      "\n",
      "\n",
      "Training with params: \n",
      "{'colsample_bytree': 0.9500000000000001, 'silent': 1, 'eval_metric': 'mlogloss', 'nthread': 4, 'min_child_weight': 3.0, 'n_estimators': 130.0, 'subsample': 0.5, 'eta': 0.125, 'objective': 'multi:softmax', 'num_class': 9, 'tree_method': 'exact', 'seed': 314159265, 'max_depth': 6, 'gamma': 0.5, 'booster': 'gbtree'}\n",
      "[0]\teval-mlogloss:1.92789\ttrain-mlogloss:1.92113\n",
      "[1]\teval-mlogloss:1.74143\ttrain-mlogloss:1.72934\n",
      "[2]\teval-mlogloss:1.59495\ttrain-mlogloss:1.57852\n",
      "[3]\teval-mlogloss:1.47371\ttrain-mlogloss:1.45474\n",
      "[4]\teval-mlogloss:1.37346\ttrain-mlogloss:1.35194\n",
      "[5]\teval-mlogloss:1.28305\ttrain-mlogloss:1.25967\n",
      "[6]\teval-mlogloss:1.20614\ttrain-mlogloss:1.18007\n",
      "[7]\teval-mlogloss:1.13975\ttrain-mlogloss:1.11183\n",
      "[8]\teval-mlogloss:1.0807\ttrain-mlogloss:1.05057\n",
      "[9]\teval-mlogloss:1.02624\ttrain-mlogloss:0.994854\n",
      "[10]\teval-mlogloss:0.978985\ttrain-mlogloss:0.946839\n",
      "[11]\teval-mlogloss:0.932916\ttrain-mlogloss:0.900608\n",
      "[12]\teval-mlogloss:0.892368\ttrain-mlogloss:0.859662\n",
      "[13]\teval-mlogloss:0.85531\ttrain-mlogloss:0.822238\n",
      "[14]\teval-mlogloss:0.823397\ttrain-mlogloss:0.789734\n",
      "[15]\teval-mlogloss:0.79303\ttrain-mlogloss:0.75955\n",
      "[16]\teval-mlogloss:0.765125\ttrain-mlogloss:0.731456\n",
      "[17]\teval-mlogloss:0.740175\ttrain-mlogloss:0.706487\n",
      "[18]\teval-mlogloss:0.717183\ttrain-mlogloss:0.68345\n",
      "[19]\teval-mlogloss:0.695516\ttrain-mlogloss:0.66164\n",
      "[20]\teval-mlogloss:0.674492\ttrain-mlogloss:0.640569\n",
      "[21]\teval-mlogloss:0.655986\ttrain-mlogloss:0.622641\n",
      "[22]\teval-mlogloss:0.638493\ttrain-mlogloss:0.605001\n",
      "[23]\teval-mlogloss:0.622749\ttrain-mlogloss:0.588831\n",
      "[24]\teval-mlogloss:0.606864\ttrain-mlogloss:0.5734\n",
      "[25]\teval-mlogloss:0.59366\ttrain-mlogloss:0.560426\n",
      "[26]\teval-mlogloss:0.579328\ttrain-mlogloss:0.545958\n",
      "[27]\teval-mlogloss:0.566982\ttrain-mlogloss:0.533347\n",
      "[28]\teval-mlogloss:0.554634\ttrain-mlogloss:0.521389\n",
      "[29]\teval-mlogloss:0.543321\ttrain-mlogloss:0.510202\n",
      "[30]\teval-mlogloss:0.532859\ttrain-mlogloss:0.499955\n",
      "[31]\teval-mlogloss:0.521898\ttrain-mlogloss:0.489097\n",
      "[32]\teval-mlogloss:0.511452\ttrain-mlogloss:0.478537\n",
      "[33]\teval-mlogloss:0.501348\ttrain-mlogloss:0.468414\n",
      "[34]\teval-mlogloss:0.492825\ttrain-mlogloss:0.459942\n",
      "[35]\teval-mlogloss:0.483348\ttrain-mlogloss:0.450887\n",
      "[36]\teval-mlogloss:0.475146\ttrain-mlogloss:0.44293\n",
      "[37]\teval-mlogloss:0.466525\ttrain-mlogloss:0.434177\n",
      "[38]\teval-mlogloss:0.458673\ttrain-mlogloss:0.426505\n",
      "[39]\teval-mlogloss:0.451264\ttrain-mlogloss:0.419337\n",
      "[40]\teval-mlogloss:0.442097\ttrain-mlogloss:0.410212\n",
      "[41]\teval-mlogloss:0.435349\ttrain-mlogloss:0.403442\n",
      "[42]\teval-mlogloss:0.428525\ttrain-mlogloss:0.396795\n",
      "[43]\teval-mlogloss:0.421748\ttrain-mlogloss:0.39012\n",
      "[44]\teval-mlogloss:0.414968\ttrain-mlogloss:0.383047\n",
      "[45]\teval-mlogloss:0.408866\ttrain-mlogloss:0.376949\n",
      "[46]\teval-mlogloss:0.403123\ttrain-mlogloss:0.371206\n",
      "[47]\teval-mlogloss:0.396697\ttrain-mlogloss:0.36499\n",
      "[48]\teval-mlogloss:0.390728\ttrain-mlogloss:0.358826\n",
      "[49]\teval-mlogloss:0.384709\ttrain-mlogloss:0.35266\n",
      "[50]\teval-mlogloss:0.380819\ttrain-mlogloss:0.348798\n",
      "[51]\teval-mlogloss:0.374608\ttrain-mlogloss:0.342568\n",
      "[52]\teval-mlogloss:0.369633\ttrain-mlogloss:0.337643\n",
      "[53]\teval-mlogloss:0.363941\ttrain-mlogloss:0.331854\n",
      "[54]\teval-mlogloss:0.358222\ttrain-mlogloss:0.326021\n",
      "[55]\teval-mlogloss:0.352024\ttrain-mlogloss:0.319775\n",
      "[56]\teval-mlogloss:0.346404\ttrain-mlogloss:0.313722\n",
      "[57]\teval-mlogloss:0.34143\ttrain-mlogloss:0.308719\n",
      "[58]\teval-mlogloss:0.336872\ttrain-mlogloss:0.304148\n",
      "[59]\teval-mlogloss:0.330147\ttrain-mlogloss:0.297439\n",
      "[60]\teval-mlogloss:0.324959\ttrain-mlogloss:0.292178\n",
      "[61]\teval-mlogloss:0.320224\ttrain-mlogloss:0.287564\n",
      "[62]\teval-mlogloss:0.314774\ttrain-mlogloss:0.282033\n",
      "[63]\teval-mlogloss:0.309622\ttrain-mlogloss:0.277189\n",
      "[64]\teval-mlogloss:0.304038\ttrain-mlogloss:0.271473\n",
      "[65]\teval-mlogloss:0.299212\ttrain-mlogloss:0.266632\n",
      "[66]\teval-mlogloss:0.294991\ttrain-mlogloss:0.262218\n",
      "[67]\teval-mlogloss:0.291009\ttrain-mlogloss:0.258173\n",
      "[68]\teval-mlogloss:0.286676\ttrain-mlogloss:0.253913\n",
      "[69]\teval-mlogloss:0.282276\ttrain-mlogloss:0.249318\n",
      "[70]\teval-mlogloss:0.277473\ttrain-mlogloss:0.244461\n",
      "[71]\teval-mlogloss:0.273733\ttrain-mlogloss:0.240793\n",
      "[72]\teval-mlogloss:0.270136\ttrain-mlogloss:0.237315\n",
      "[73]\teval-mlogloss:0.266618\ttrain-mlogloss:0.233829\n",
      "[74]\teval-mlogloss:0.262503\ttrain-mlogloss:0.22939\n",
      "[75]\teval-mlogloss:0.259873\ttrain-mlogloss:0.226716\n",
      "[76]\teval-mlogloss:0.254013\ttrain-mlogloss:0.221095\n",
      "[77]\teval-mlogloss:0.249635\ttrain-mlogloss:0.216514\n",
      "[78]\teval-mlogloss:0.245839\ttrain-mlogloss:0.2126\n",
      "[79]\teval-mlogloss:0.242239\ttrain-mlogloss:0.208699\n",
      "[80]\teval-mlogloss:0.237601\ttrain-mlogloss:0.203929\n",
      "[81]\teval-mlogloss:0.234135\ttrain-mlogloss:0.200337\n",
      "[82]\teval-mlogloss:0.229477\ttrain-mlogloss:0.195704\n",
      "[83]\teval-mlogloss:0.226223\ttrain-mlogloss:0.19273\n",
      "[84]\teval-mlogloss:0.223124\ttrain-mlogloss:0.189368\n",
      "[85]\teval-mlogloss:0.218618\ttrain-mlogloss:0.184952\n",
      "[86]\teval-mlogloss:0.214466\ttrain-mlogloss:0.180591\n",
      "[87]\teval-mlogloss:0.210619\ttrain-mlogloss:0.176713\n",
      "[88]\teval-mlogloss:0.206594\ttrain-mlogloss:0.172659\n",
      "[89]\teval-mlogloss:0.202847\ttrain-mlogloss:0.169023\n",
      "[90]\teval-mlogloss:0.199469\ttrain-mlogloss:0.165527\n",
      "[91]\teval-mlogloss:0.195479\ttrain-mlogloss:0.161528\n",
      "[92]\teval-mlogloss:0.192669\ttrain-mlogloss:0.1586\n",
      "[93]\teval-mlogloss:0.190107\ttrain-mlogloss:0.15612\n",
      "[94]\teval-mlogloss:0.186383\ttrain-mlogloss:0.152612\n",
      "[95]\teval-mlogloss:0.183059\ttrain-mlogloss:0.149182\n",
      "[96]\teval-mlogloss:0.179578\ttrain-mlogloss:0.145782\n",
      "[97]\teval-mlogloss:0.177031\ttrain-mlogloss:0.143446\n",
      "[98]\teval-mlogloss:0.174724\ttrain-mlogloss:0.141107\n",
      "[99]\teval-mlogloss:0.172259\ttrain-mlogloss:0.138671\n",
      "[100]\teval-mlogloss:0.170095\ttrain-mlogloss:0.136556\n",
      "[101]\teval-mlogloss:0.167157\ttrain-mlogloss:0.133591\n",
      "[102]\teval-mlogloss:0.164985\ttrain-mlogloss:0.131501\n",
      "[103]\teval-mlogloss:0.162551\ttrain-mlogloss:0.129025\n",
      "[104]\teval-mlogloss:0.160533\ttrain-mlogloss:0.127046\n",
      "[105]\teval-mlogloss:0.158103\ttrain-mlogloss:0.124521\n",
      "[106]\teval-mlogloss:0.155907\ttrain-mlogloss:0.122441\n",
      "[107]\teval-mlogloss:0.15401\ttrain-mlogloss:0.120553\n",
      "[108]\teval-mlogloss:0.151612\ttrain-mlogloss:0.118308\n",
      "[109]\teval-mlogloss:0.149458\ttrain-mlogloss:0.116192\n",
      "[110]\teval-mlogloss:0.14762\ttrain-mlogloss:0.11443\n",
      "[111]\teval-mlogloss:0.145525\ttrain-mlogloss:0.112536\n",
      "[112]\teval-mlogloss:0.143527\ttrain-mlogloss:0.110399\n",
      "[113]\teval-mlogloss:0.141293\ttrain-mlogloss:0.108321\n",
      "[114]\teval-mlogloss:0.139064\ttrain-mlogloss:0.106285\n",
      "[115]\teval-mlogloss:0.137502\ttrain-mlogloss:0.104795\n",
      "[116]\teval-mlogloss:0.135344\ttrain-mlogloss:0.102843\n",
      "[117]\teval-mlogloss:0.13299\ttrain-mlogloss:0.10063\n",
      "[118]\teval-mlogloss:0.131504\ttrain-mlogloss:0.099218\n",
      "[119]\teval-mlogloss:0.129841\ttrain-mlogloss:0.097641\n",
      "[120]\teval-mlogloss:0.128114\ttrain-mlogloss:0.096126\n",
      "[121]\teval-mlogloss:0.126309\ttrain-mlogloss:0.09441\n",
      "[122]\teval-mlogloss:0.124784\ttrain-mlogloss:0.092931\n",
      "[123]\teval-mlogloss:0.122924\ttrain-mlogloss:0.091358\n",
      "[124]\teval-mlogloss:0.120719\ttrain-mlogloss:0.089282\n",
      "[125]\teval-mlogloss:0.118647\ttrain-mlogloss:0.087392\n",
      "[126]\teval-mlogloss:0.117512\ttrain-mlogloss:0.086096\n",
      "[127]\teval-mlogloss:0.115977\ttrain-mlogloss:0.08465\n",
      "[128]\teval-mlogloss:0.114131\ttrain-mlogloss:0.082792\n",
      "[129]\teval-mlogloss:0.112955\ttrain-mlogloss:0.081714\n",
      "\tScore 0.985850035271\n",
      "\n",
      "\n",
      "Training with params: \n",
      "{'colsample_bytree': 0.55, 'silent': 1, 'eval_metric': 'mlogloss', 'nthread': 4, 'min_child_weight': 2.0, 'n_estimators': 117.0, 'subsample': 0.9500000000000001, 'eta': 0.42500000000000004, 'objective': 'multi:softmax', 'num_class': 9, 'tree_method': 'exact', 'seed': 314159265, 'max_depth': 6, 'gamma': 0.75, 'booster': 'gbtree'}\n",
      "[0]\teval-mlogloss:1.44469\ttrain-mlogloss:1.41586\n",
      "[1]\teval-mlogloss:1.17193\ttrain-mlogloss:1.13522\n",
      "[2]\teval-mlogloss:0.986545\ttrain-mlogloss:0.943008\n",
      "[3]\teval-mlogloss:0.860391\ttrain-mlogloss:0.816106\n",
      "[4]\teval-mlogloss:0.761685\ttrain-mlogloss:0.716623\n",
      "[5]\teval-mlogloss:0.684395\ttrain-mlogloss:0.639716\n",
      "[6]\teval-mlogloss:0.621523\ttrain-mlogloss:0.578735\n",
      "[7]\teval-mlogloss:0.573709\ttrain-mlogloss:0.530881\n",
      "[8]\teval-mlogloss:0.53843\ttrain-mlogloss:0.495267\n",
      "[9]\teval-mlogloss:0.504356\ttrain-mlogloss:0.460889\n",
      "[10]\teval-mlogloss:0.469825\ttrain-mlogloss:0.426732\n",
      "[11]\teval-mlogloss:0.443554\ttrain-mlogloss:0.400875\n",
      "[12]\teval-mlogloss:0.420191\ttrain-mlogloss:0.378074\n",
      "[13]\teval-mlogloss:0.39945\ttrain-mlogloss:0.357973\n",
      "[14]\teval-mlogloss:0.378798\ttrain-mlogloss:0.337619\n",
      "[15]\teval-mlogloss:0.361147\ttrain-mlogloss:0.320762\n",
      "[16]\teval-mlogloss:0.342794\ttrain-mlogloss:0.301609\n",
      "[17]\teval-mlogloss:0.323657\ttrain-mlogloss:0.283237\n",
      "[18]\teval-mlogloss:0.308333\ttrain-mlogloss:0.266414\n",
      "[19]\teval-mlogloss:0.293466\ttrain-mlogloss:0.251337\n",
      "[20]\teval-mlogloss:0.281558\ttrain-mlogloss:0.238856\n",
      "[21]\teval-mlogloss:0.269958\ttrain-mlogloss:0.227889\n",
      "[22]\teval-mlogloss:0.260351\ttrain-mlogloss:0.218506\n",
      "[23]\teval-mlogloss:0.251334\ttrain-mlogloss:0.2096\n",
      "[24]\teval-mlogloss:0.236967\ttrain-mlogloss:0.194475\n",
      "[25]\teval-mlogloss:0.226366\ttrain-mlogloss:0.182709\n",
      "[26]\teval-mlogloss:0.217202\ttrain-mlogloss:0.173542\n",
      "[27]\teval-mlogloss:0.206886\ttrain-mlogloss:0.162952\n",
      "[28]\teval-mlogloss:0.19607\ttrain-mlogloss:0.152063\n",
      "[29]\teval-mlogloss:0.187501\ttrain-mlogloss:0.14361\n",
      "[30]\teval-mlogloss:0.1793\ttrain-mlogloss:0.135822\n",
      "[31]\teval-mlogloss:0.166798\ttrain-mlogloss:0.124141\n",
      "[32]\teval-mlogloss:0.159243\ttrain-mlogloss:0.117409\n",
      "[33]\teval-mlogloss:0.152074\ttrain-mlogloss:0.110558\n",
      "[34]\teval-mlogloss:0.144023\ttrain-mlogloss:0.102845\n",
      "[35]\teval-mlogloss:0.13675\ttrain-mlogloss:0.096394\n",
      "[36]\teval-mlogloss:0.131577\ttrain-mlogloss:0.091318\n",
      "[37]\teval-mlogloss:0.12553\ttrain-mlogloss:0.085836\n",
      "[38]\teval-mlogloss:0.119122\ttrain-mlogloss:0.080076\n",
      "[39]\teval-mlogloss:0.113028\ttrain-mlogloss:0.074467\n",
      "[40]\teval-mlogloss:0.107772\ttrain-mlogloss:0.070087\n",
      "[41]\teval-mlogloss:0.102448\ttrain-mlogloss:0.06531\n",
      "[42]\teval-mlogloss:0.098517\ttrain-mlogloss:0.062047\n",
      "[43]\teval-mlogloss:0.094599\ttrain-mlogloss:0.058658\n",
      "[44]\teval-mlogloss:0.09125\ttrain-mlogloss:0.055896\n",
      "[45]\teval-mlogloss:0.087873\ttrain-mlogloss:0.053087\n",
      "[46]\teval-mlogloss:0.083506\ttrain-mlogloss:0.04973\n",
      "[47]\teval-mlogloss:0.081277\ttrain-mlogloss:0.047863\n",
      "[48]\teval-mlogloss:0.077665\ttrain-mlogloss:0.044746\n",
      "[49]\teval-mlogloss:0.07537\ttrain-mlogloss:0.04277\n",
      "[50]\teval-mlogloss:0.071826\ttrain-mlogloss:0.040171\n",
      "[51]\teval-mlogloss:0.069458\ttrain-mlogloss:0.038443\n",
      "[52]\teval-mlogloss:0.06684\ttrain-mlogloss:0.036258\n",
      "[53]\teval-mlogloss:0.065283\ttrain-mlogloss:0.034853\n",
      "[54]\teval-mlogloss:0.063765\ttrain-mlogloss:0.033463\n",
      "[55]\teval-mlogloss:0.062168\ttrain-mlogloss:0.032256\n",
      "[56]\teval-mlogloss:0.060171\ttrain-mlogloss:0.030683\n",
      "[57]\teval-mlogloss:0.058071\ttrain-mlogloss:0.029274\n",
      "[58]\teval-mlogloss:0.056495\ttrain-mlogloss:0.028018\n",
      "[59]\teval-mlogloss:0.054951\ttrain-mlogloss:0.026845\n",
      "[60]\teval-mlogloss:0.053739\ttrain-mlogloss:0.025963\n",
      "[61]\teval-mlogloss:0.051921\ttrain-mlogloss:0.02457\n",
      "[62]\teval-mlogloss:0.05081\ttrain-mlogloss:0.023786\n",
      "[63]\teval-mlogloss:0.049445\ttrain-mlogloss:0.022786\n",
      "[64]\teval-mlogloss:0.048847\ttrain-mlogloss:0.02233\n",
      "[65]\teval-mlogloss:0.047858\ttrain-mlogloss:0.021529\n",
      "[66]\teval-mlogloss:0.046979\ttrain-mlogloss:0.020879\n",
      "[67]\teval-mlogloss:0.045875\ttrain-mlogloss:0.020085\n",
      "[68]\teval-mlogloss:0.044899\ttrain-mlogloss:0.019264\n",
      "[69]\teval-mlogloss:0.044387\ttrain-mlogloss:0.018948\n",
      "[70]\teval-mlogloss:0.043987\ttrain-mlogloss:0.018604\n",
      "[71]\teval-mlogloss:0.04315\ttrain-mlogloss:0.018044\n",
      "[72]\teval-mlogloss:0.042621\ttrain-mlogloss:0.017658\n",
      "[73]\teval-mlogloss:0.041861\ttrain-mlogloss:0.01697\n",
      "[74]\teval-mlogloss:0.041094\ttrain-mlogloss:0.016369\n",
      "[75]\teval-mlogloss:0.04028\ttrain-mlogloss:0.015707\n",
      "[76]\teval-mlogloss:0.039965\ttrain-mlogloss:0.015448\n",
      "[77]\teval-mlogloss:0.039632\ttrain-mlogloss:0.015222\n",
      "[78]\teval-mlogloss:0.038911\ttrain-mlogloss:0.014781\n",
      "[79]\teval-mlogloss:0.03842\ttrain-mlogloss:0.014384\n",
      "[80]\teval-mlogloss:0.03805\ttrain-mlogloss:0.014114\n",
      "[81]\teval-mlogloss:0.037419\ttrain-mlogloss:0.013699\n",
      "[82]\teval-mlogloss:0.037129\ttrain-mlogloss:0.013481\n",
      "[83]\teval-mlogloss:0.036864\ttrain-mlogloss:0.013305\n",
      "[84]\teval-mlogloss:0.036463\ttrain-mlogloss:0.013133\n",
      "[85]\teval-mlogloss:0.035883\ttrain-mlogloss:0.012762\n",
      "[86]\teval-mlogloss:0.035613\ttrain-mlogloss:0.012599\n",
      "[87]\teval-mlogloss:0.035061\ttrain-mlogloss:0.012273\n",
      "[88]\teval-mlogloss:0.034732\ttrain-mlogloss:0.012078\n",
      "[89]\teval-mlogloss:0.034434\ttrain-mlogloss:0.011897\n",
      "[90]\teval-mlogloss:0.034151\ttrain-mlogloss:0.011678\n",
      "[91]\teval-mlogloss:0.033896\ttrain-mlogloss:0.011505\n",
      "[92]\teval-mlogloss:0.033735\ttrain-mlogloss:0.011404\n",
      "[93]\teval-mlogloss:0.0337\ttrain-mlogloss:0.011325\n",
      "[94]\teval-mlogloss:0.033574\ttrain-mlogloss:0.011257\n",
      "[95]\teval-mlogloss:0.03329\ttrain-mlogloss:0.011102\n",
      "[96]\teval-mlogloss:0.033183\ttrain-mlogloss:0.010994\n",
      "[97]\teval-mlogloss:0.033044\ttrain-mlogloss:0.010939\n",
      "[98]\teval-mlogloss:0.032857\ttrain-mlogloss:0.010844\n",
      "[99]\teval-mlogloss:0.032747\ttrain-mlogloss:0.01078\n",
      "[100]\teval-mlogloss:0.032654\ttrain-mlogloss:0.010734\n",
      "[101]\teval-mlogloss:0.032537\ttrain-mlogloss:0.010633\n",
      "[102]\teval-mlogloss:0.032316\ttrain-mlogloss:0.0105\n",
      "[103]\teval-mlogloss:0.03222\ttrain-mlogloss:0.010461\n",
      "[104]\teval-mlogloss:0.03187\ttrain-mlogloss:0.01028\n",
      "[105]\teval-mlogloss:0.031537\ttrain-mlogloss:0.01012\n",
      "[106]\teval-mlogloss:0.031483\ttrain-mlogloss:0.010093\n",
      "[107]\teval-mlogloss:0.031212\ttrain-mlogloss:0.009981\n",
      "[108]\teval-mlogloss:0.031192\ttrain-mlogloss:0.009946\n",
      "[109]\teval-mlogloss:0.031041\ttrain-mlogloss:0.009894\n",
      "[110]\teval-mlogloss:0.031006\ttrain-mlogloss:0.009868\n",
      "[111]\teval-mlogloss:0.030843\ttrain-mlogloss:0.009784\n",
      "[112]\teval-mlogloss:0.030838\ttrain-mlogloss:0.009766\n",
      "[113]\teval-mlogloss:0.030819\ttrain-mlogloss:0.009757\n",
      "[114]\teval-mlogloss:0.030726\ttrain-mlogloss:0.009704\n",
      "[115]\teval-mlogloss:0.030594\ttrain-mlogloss:0.009666\n",
      "[116]\teval-mlogloss:0.03059\ttrain-mlogloss:0.00966\n",
      "\tScore 0.99485455828\n",
      "\n",
      "\n",
      "Training with params: \n",
      "{'colsample_bytree': 0.9, 'silent': 1, 'eval_metric': 'mlogloss', 'nthread': 4, 'min_child_weight': 4.0, 'n_estimators': 122.0, 'subsample': 0.6000000000000001, 'eta': 0.05, 'objective': 'multi:softmax', 'num_class': 9, 'tree_method': 'exact', 'seed': 314159265, 'max_depth': 4, 'gamma': 0.8, 'booster': 'gbtree'}\n",
      "[0]\teval-mlogloss:2.11124\ttrain-mlogloss:2.11045\n",
      "[1]\teval-mlogloss:2.03859\ttrain-mlogloss:2.03638\n",
      "[2]\teval-mlogloss:1.97369\ttrain-mlogloss:1.97042\n",
      "[3]\teval-mlogloss:1.91449\ttrain-mlogloss:1.90983\n",
      "[4]\teval-mlogloss:1.86084\ttrain-mlogloss:1.85515\n",
      "[5]\teval-mlogloss:1.81035\ttrain-mlogloss:1.80414\n",
      "[6]\teval-mlogloss:1.76298\ttrain-mlogloss:1.75614\n",
      "[7]\teval-mlogloss:1.72005\ttrain-mlogloss:1.71261\n",
      "[8]\teval-mlogloss:1.68019\ttrain-mlogloss:1.67196\n",
      "[9]\teval-mlogloss:1.64229\ttrain-mlogloss:1.63315\n",
      "[10]\teval-mlogloss:1.60703\ttrain-mlogloss:1.59729\n",
      "[11]\teval-mlogloss:1.57416\ttrain-mlogloss:1.56353\n",
      "[12]\teval-mlogloss:1.54277\ttrain-mlogloss:1.53131\n",
      "[13]\teval-mlogloss:1.51388\ttrain-mlogloss:1.50196\n",
      "[14]\teval-mlogloss:1.48692\ttrain-mlogloss:1.47406\n",
      "[15]\teval-mlogloss:1.46083\ttrain-mlogloss:1.44712\n",
      "[16]\teval-mlogloss:1.43588\ttrain-mlogloss:1.42119\n",
      "[17]\teval-mlogloss:1.41195\ttrain-mlogloss:1.39633\n",
      "[18]\teval-mlogloss:1.38915\ttrain-mlogloss:1.37316\n",
      "[19]\teval-mlogloss:1.36734\ttrain-mlogloss:1.35031\n",
      "[20]\teval-mlogloss:1.34671\ttrain-mlogloss:1.32883\n",
      "[21]\teval-mlogloss:1.32691\ttrain-mlogloss:1.30857\n",
      "[22]\teval-mlogloss:1.30786\ttrain-mlogloss:1.28876\n",
      "[23]\teval-mlogloss:1.28995\ttrain-mlogloss:1.27013\n",
      "[24]\teval-mlogloss:1.27235\ttrain-mlogloss:1.25201\n",
      "[25]\teval-mlogloss:1.25522\ttrain-mlogloss:1.23457\n",
      "[26]\teval-mlogloss:1.2386\ttrain-mlogloss:1.21778\n",
      "[27]\teval-mlogloss:1.22228\ttrain-mlogloss:1.20112\n",
      "[28]\teval-mlogloss:1.20716\ttrain-mlogloss:1.18562\n",
      "[29]\teval-mlogloss:1.19228\ttrain-mlogloss:1.1708\n",
      "[30]\teval-mlogloss:1.17748\ttrain-mlogloss:1.1558\n",
      "[31]\teval-mlogloss:1.16389\ttrain-mlogloss:1.14209\n",
      "[32]\teval-mlogloss:1.15025\ttrain-mlogloss:1.12808\n",
      "[33]\teval-mlogloss:1.13757\ttrain-mlogloss:1.11511\n",
      "[34]\teval-mlogloss:1.12524\ttrain-mlogloss:1.10234\n",
      "[35]\teval-mlogloss:1.11334\ttrain-mlogloss:1.09022\n",
      "[36]\teval-mlogloss:1.10234\ttrain-mlogloss:1.07895\n",
      "[37]\teval-mlogloss:1.09115\ttrain-mlogloss:1.06759\n",
      "[38]\teval-mlogloss:1.08069\ttrain-mlogloss:1.05708\n",
      "[39]\teval-mlogloss:1.07011\ttrain-mlogloss:1.04619\n",
      "[40]\teval-mlogloss:1.05953\ttrain-mlogloss:1.03554\n",
      "[41]\teval-mlogloss:1.04988\ttrain-mlogloss:1.02569\n",
      "[42]\teval-mlogloss:1.04056\ttrain-mlogloss:1.0164\n",
      "[43]\teval-mlogloss:1.03146\ttrain-mlogloss:1.00704\n",
      "[44]\teval-mlogloss:1.02258\ttrain-mlogloss:0.998012\n",
      "[45]\teval-mlogloss:1.01414\ttrain-mlogloss:0.989314\n",
      "[46]\teval-mlogloss:1.00579\ttrain-mlogloss:0.980936\n",
      "[47]\teval-mlogloss:0.997731\ttrain-mlogloss:0.972935\n",
      "[48]\teval-mlogloss:0.989834\ttrain-mlogloss:0.965023\n",
      "[49]\teval-mlogloss:0.982429\ttrain-mlogloss:0.957686\n",
      "[50]\teval-mlogloss:0.975079\ttrain-mlogloss:0.950463\n",
      "[51]\teval-mlogloss:0.968175\ttrain-mlogloss:0.943529\n",
      "[52]\teval-mlogloss:0.961294\ttrain-mlogloss:0.936699\n",
      "[53]\teval-mlogloss:0.954523\ttrain-mlogloss:0.929798\n",
      "[54]\teval-mlogloss:0.947252\ttrain-mlogloss:0.922463\n",
      "[55]\teval-mlogloss:0.940576\ttrain-mlogloss:0.915937\n",
      "[56]\teval-mlogloss:0.934156\ttrain-mlogloss:0.909606\n",
      "[57]\teval-mlogloss:0.92801\ttrain-mlogloss:0.903408\n",
      "[58]\teval-mlogloss:0.921853\ttrain-mlogloss:0.897206\n",
      "[59]\teval-mlogloss:0.91565\ttrain-mlogloss:0.890832\n",
      "[60]\teval-mlogloss:0.909917\ttrain-mlogloss:0.885109\n",
      "[61]\teval-mlogloss:0.904092\ttrain-mlogloss:0.879381\n",
      "[62]\teval-mlogloss:0.898742\ttrain-mlogloss:0.873992\n",
      "[63]\teval-mlogloss:0.893321\ttrain-mlogloss:0.868505\n",
      "[64]\teval-mlogloss:0.887929\ttrain-mlogloss:0.863103\n",
      "[65]\teval-mlogloss:0.882554\ttrain-mlogloss:0.857874\n",
      "[66]\teval-mlogloss:0.877429\ttrain-mlogloss:0.852866\n",
      "[67]\teval-mlogloss:0.872159\ttrain-mlogloss:0.847537\n",
      "[68]\teval-mlogloss:0.867255\ttrain-mlogloss:0.842538\n",
      "[69]\teval-mlogloss:0.862554\ttrain-mlogloss:0.837775\n",
      "[70]\teval-mlogloss:0.857781\ttrain-mlogloss:0.83289\n",
      "[71]\teval-mlogloss:0.853161\ttrain-mlogloss:0.828362\n",
      "[72]\teval-mlogloss:0.848621\ttrain-mlogloss:0.823769\n",
      "[73]\teval-mlogloss:0.844344\ttrain-mlogloss:0.819451\n",
      "[74]\teval-mlogloss:0.840193\ttrain-mlogloss:0.815262\n",
      "[75]\teval-mlogloss:0.835886\ttrain-mlogloss:0.811046\n",
      "[76]\teval-mlogloss:0.83153\ttrain-mlogloss:0.806908\n",
      "[77]\teval-mlogloss:0.827361\ttrain-mlogloss:0.802625\n",
      "[78]\teval-mlogloss:0.823179\ttrain-mlogloss:0.798491\n",
      "[79]\teval-mlogloss:0.819562\ttrain-mlogloss:0.794775\n",
      "[80]\teval-mlogloss:0.815588\ttrain-mlogloss:0.79092\n",
      "[81]\teval-mlogloss:0.811614\ttrain-mlogloss:0.786875\n",
      "[82]\teval-mlogloss:0.807671\ttrain-mlogloss:0.782808\n",
      "[83]\teval-mlogloss:0.804046\ttrain-mlogloss:0.779307\n",
      "[84]\teval-mlogloss:0.799997\ttrain-mlogloss:0.775321\n",
      "[85]\teval-mlogloss:0.796373\ttrain-mlogloss:0.77182\n",
      "[86]\teval-mlogloss:0.792859\ttrain-mlogloss:0.76827\n",
      "[87]\teval-mlogloss:0.788935\ttrain-mlogloss:0.764285\n",
      "[88]\teval-mlogloss:0.785616\ttrain-mlogloss:0.760846\n",
      "[89]\teval-mlogloss:0.782346\ttrain-mlogloss:0.757666\n",
      "[90]\teval-mlogloss:0.77852\ttrain-mlogloss:0.753971\n",
      "[91]\teval-mlogloss:0.775376\ttrain-mlogloss:0.750848\n",
      "[92]\teval-mlogloss:0.77243\ttrain-mlogloss:0.747971\n",
      "[93]\teval-mlogloss:0.768688\ttrain-mlogloss:0.744272\n",
      "[94]\teval-mlogloss:0.765881\ttrain-mlogloss:0.741433\n",
      "[95]\teval-mlogloss:0.762257\ttrain-mlogloss:0.737806\n",
      "[96]\teval-mlogloss:0.759132\ttrain-mlogloss:0.734774\n",
      "[97]\teval-mlogloss:0.755719\ttrain-mlogloss:0.73128\n",
      "[98]\teval-mlogloss:0.753011\ttrain-mlogloss:0.72865\n",
      "[99]\teval-mlogloss:0.750302\ttrain-mlogloss:0.726054\n",
      "[100]\teval-mlogloss:0.747685\ttrain-mlogloss:0.723404\n",
      "[101]\teval-mlogloss:0.744771\ttrain-mlogloss:0.720547\n",
      "[102]\teval-mlogloss:0.741858\ttrain-mlogloss:0.717617\n",
      "[103]\teval-mlogloss:0.739241\ttrain-mlogloss:0.715021\n",
      "[104]\teval-mlogloss:0.736166\ttrain-mlogloss:0.711851\n",
      "[105]\teval-mlogloss:0.733202\ttrain-mlogloss:0.708994\n",
      "[106]\teval-mlogloss:0.730148\ttrain-mlogloss:0.705729\n",
      "[107]\teval-mlogloss:0.727643\ttrain-mlogloss:0.703287\n",
      "[108]\teval-mlogloss:0.725243\ttrain-mlogloss:0.700918\n",
      "[109]\teval-mlogloss:0.722467\ttrain-mlogloss:0.698127\n",
      "[110]\teval-mlogloss:0.72018\ttrain-mlogloss:0.695752\n",
      "[111]\teval-mlogloss:0.716916\ttrain-mlogloss:0.692475\n",
      "[112]\teval-mlogloss:0.714134\ttrain-mlogloss:0.689736\n",
      "[113]\teval-mlogloss:0.711753\ttrain-mlogloss:0.687321\n",
      "[114]\teval-mlogloss:0.709637\ttrain-mlogloss:0.685305\n",
      "[115]\teval-mlogloss:0.707224\ttrain-mlogloss:0.682917\n",
      "[116]\teval-mlogloss:0.70454\ttrain-mlogloss:0.680312\n",
      "[117]\teval-mlogloss:0.702331\ttrain-mlogloss:0.678075\n",
      "[118]\teval-mlogloss:0.700121\ttrain-mlogloss:0.675841\n",
      "[119]\teval-mlogloss:0.697599\ttrain-mlogloss:0.673163\n",
      "[120]\teval-mlogloss:0.695358\ttrain-mlogloss:0.670933\n",
      "[121]\teval-mlogloss:0.693005\ttrain-mlogloss:0.668471\n",
      "\tScore 0.768206149633\n",
      "\n",
      "\n",
      "Training with params: \n",
      "{'colsample_bytree': 0.75, 'silent': 1, 'eval_metric': 'mlogloss', 'nthread': 4, 'min_child_weight': 5.0, 'n_estimators': 111.0, 'subsample': 0.9500000000000001, 'eta': 0.05, 'objective': 'multi:softmax', 'num_class': 9, 'tree_method': 'exact', 'seed': 314159265, 'max_depth': 11, 'gamma': 0.6000000000000001, 'booster': 'gbtree'}\n",
      "[0]\teval-mlogloss:2.03446\ttrain-mlogloss:2.02775\n",
      "[1]\teval-mlogloss:1.90144\ttrain-mlogloss:1.887\n",
      "[2]\teval-mlogloss:1.78159\ttrain-mlogloss:1.76181\n",
      "[3]\teval-mlogloss:1.67796\ttrain-mlogloss:1.65274\n",
      "[4]\teval-mlogloss:1.58725\ttrain-mlogloss:1.55808\n",
      "[5]\teval-mlogloss:1.50414\ttrain-mlogloss:1.47117\n",
      "[6]\teval-mlogloss:1.42699\ttrain-mlogloss:1.39104\n",
      "[7]\teval-mlogloss:1.35763\ttrain-mlogloss:1.31881\n",
      "[8]\teval-mlogloss:1.29347\ttrain-mlogloss:1.25197\n",
      "[9]\teval-mlogloss:1.23314\ttrain-mlogloss:1.18961\n",
      "[10]\teval-mlogloss:1.17794\ttrain-mlogloss:1.13249\n",
      "[11]\teval-mlogloss:1.12824\ttrain-mlogloss:1.08104\n",
      "[12]\teval-mlogloss:1.07975\ttrain-mlogloss:1.03109\n",
      "[13]\teval-mlogloss:1.03509\ttrain-mlogloss:0.985164\n",
      "[14]\teval-mlogloss:0.991645\ttrain-mlogloss:0.940336\n",
      "[15]\teval-mlogloss:0.952656\ttrain-mlogloss:0.899988\n",
      "[16]\teval-mlogloss:0.914948\ttrain-mlogloss:0.861099\n",
      "[17]\teval-mlogloss:0.878774\ttrain-mlogloss:0.824275\n",
      "[18]\teval-mlogloss:0.845551\ttrain-mlogloss:0.790457\n",
      "[19]\teval-mlogloss:0.813629\ttrain-mlogloss:0.758231\n",
      "[20]\teval-mlogloss:0.783945\ttrain-mlogloss:0.727982\n",
      "[21]\teval-mlogloss:0.754889\ttrain-mlogloss:0.69845\n",
      "[22]\teval-mlogloss:0.727716\ttrain-mlogloss:0.670841\n",
      "[23]\teval-mlogloss:0.701214\ttrain-mlogloss:0.643801\n",
      "[24]\teval-mlogloss:0.676292\ttrain-mlogloss:0.618775\n",
      "[25]\teval-mlogloss:0.652963\ttrain-mlogloss:0.595203\n",
      "[26]\teval-mlogloss:0.630865\ttrain-mlogloss:0.573073\n",
      "[27]\teval-mlogloss:0.609622\ttrain-mlogloss:0.551612\n",
      "[28]\teval-mlogloss:0.589202\ttrain-mlogloss:0.531131\n",
      "[29]\teval-mlogloss:0.569512\ttrain-mlogloss:0.511373\n",
      "[30]\teval-mlogloss:0.551307\ttrain-mlogloss:0.492872\n",
      "[31]\teval-mlogloss:0.532995\ttrain-mlogloss:0.474589\n",
      "[32]\teval-mlogloss:0.515849\ttrain-mlogloss:0.457602\n",
      "[33]\teval-mlogloss:0.499246\ttrain-mlogloss:0.440991\n",
      "[34]\teval-mlogloss:0.483824\ttrain-mlogloss:0.425514\n",
      "[35]\teval-mlogloss:0.469116\ttrain-mlogloss:0.410902\n",
      "[36]\teval-mlogloss:0.454707\ttrain-mlogloss:0.396713\n",
      "[37]\teval-mlogloss:0.440984\ttrain-mlogloss:0.383148\n",
      "[38]\teval-mlogloss:0.427716\ttrain-mlogloss:0.370108\n",
      "[39]\teval-mlogloss:0.41439\ttrain-mlogloss:0.357031\n",
      "[40]\teval-mlogloss:0.401768\ttrain-mlogloss:0.344558\n",
      "[41]\teval-mlogloss:0.390108\ttrain-mlogloss:0.333136\n",
      "[42]\teval-mlogloss:0.379102\ttrain-mlogloss:0.322379\n",
      "[43]\teval-mlogloss:0.367946\ttrain-mlogloss:0.311518\n",
      "[44]\teval-mlogloss:0.357047\ttrain-mlogloss:0.300866\n",
      "[45]\teval-mlogloss:0.34719\ttrain-mlogloss:0.291151\n",
      "[46]\teval-mlogloss:0.337781\ttrain-mlogloss:0.282026\n",
      "[47]\teval-mlogloss:0.328786\ttrain-mlogloss:0.273245\n",
      "[48]\teval-mlogloss:0.320199\ttrain-mlogloss:0.264949\n",
      "[49]\teval-mlogloss:0.312235\ttrain-mlogloss:0.257292\n",
      "[50]\teval-mlogloss:0.303435\ttrain-mlogloss:0.248937\n",
      "[51]\teval-mlogloss:0.295131\ttrain-mlogloss:0.241009\n",
      "[52]\teval-mlogloss:0.287726\ttrain-mlogloss:0.233949\n",
      "[53]\teval-mlogloss:0.28008\ttrain-mlogloss:0.226693\n",
      "[54]\teval-mlogloss:0.272578\ttrain-mlogloss:0.21952\n",
      "[55]\teval-mlogloss:0.265978\ttrain-mlogloss:0.213237\n",
      "[56]\teval-mlogloss:0.25963\ttrain-mlogloss:0.207253\n",
      "[57]\teval-mlogloss:0.252913\ttrain-mlogloss:0.200903\n",
      "[58]\teval-mlogloss:0.246327\ttrain-mlogloss:0.194619\n",
      "[59]\teval-mlogloss:0.239919\ttrain-mlogloss:0.188557\n",
      "[60]\teval-mlogloss:0.233828\ttrain-mlogloss:0.182778\n",
      "[61]\teval-mlogloss:0.228124\ttrain-mlogloss:0.177394\n",
      "[62]\teval-mlogloss:0.222376\ttrain-mlogloss:0.172052\n",
      "[63]\teval-mlogloss:0.216752\ttrain-mlogloss:0.166842\n",
      "[64]\teval-mlogloss:0.211886\ttrain-mlogloss:0.162384\n",
      "[65]\teval-mlogloss:0.207191\ttrain-mlogloss:0.158042\n",
      "[66]\teval-mlogloss:0.202443\ttrain-mlogloss:0.153675\n",
      "[67]\teval-mlogloss:0.197674\ttrain-mlogloss:0.149255\n",
      "[68]\teval-mlogloss:0.193472\ttrain-mlogloss:0.145341\n",
      "[69]\teval-mlogloss:0.189116\ttrain-mlogloss:0.141286\n",
      "[70]\teval-mlogloss:0.184565\ttrain-mlogloss:0.137142\n",
      "[71]\teval-mlogloss:0.180243\ttrain-mlogloss:0.133238\n",
      "[72]\teval-mlogloss:0.176328\ttrain-mlogloss:0.129676\n",
      "[73]\teval-mlogloss:0.172676\ttrain-mlogloss:0.126376\n",
      "[74]\teval-mlogloss:0.169237\ttrain-mlogloss:0.123234\n",
      "[75]\teval-mlogloss:0.165685\ttrain-mlogloss:0.120074\n",
      "[76]\teval-mlogloss:0.162179\ttrain-mlogloss:0.116875\n",
      "[77]\teval-mlogloss:0.159039\ttrain-mlogloss:0.114072\n",
      "[78]\teval-mlogloss:0.155939\ttrain-mlogloss:0.111254\n",
      "[79]\teval-mlogloss:0.153069\ttrain-mlogloss:0.108645\n",
      "[80]\teval-mlogloss:0.15001\ttrain-mlogloss:0.105904\n",
      "[81]\teval-mlogloss:0.147077\ttrain-mlogloss:0.103294\n",
      "[82]\teval-mlogloss:0.1441\ttrain-mlogloss:0.100661\n",
      "[83]\teval-mlogloss:0.141159\ttrain-mlogloss:0.098088\n",
      "[84]\teval-mlogloss:0.13857\ttrain-mlogloss:0.095862\n",
      "[85]\teval-mlogloss:0.135791\ttrain-mlogloss:0.093427\n",
      "[86]\teval-mlogloss:0.13313\ttrain-mlogloss:0.091148\n",
      "[87]\teval-mlogloss:0.130765\ttrain-mlogloss:0.08904\n",
      "[88]\teval-mlogloss:0.128433\ttrain-mlogloss:0.087009\n",
      "[89]\teval-mlogloss:0.126084\ttrain-mlogloss:0.084956\n",
      "[90]\teval-mlogloss:0.123997\ttrain-mlogloss:0.083098\n",
      "[91]\teval-mlogloss:0.12195\ttrain-mlogloss:0.081321\n",
      "[92]\teval-mlogloss:0.119776\ttrain-mlogloss:0.079413\n",
      "[93]\teval-mlogloss:0.117661\ttrain-mlogloss:0.07763\n",
      "[94]\teval-mlogloss:0.115623\ttrain-mlogloss:0.075888\n",
      "[95]\teval-mlogloss:0.113576\ttrain-mlogloss:0.074135\n",
      "[96]\teval-mlogloss:0.111627\ttrain-mlogloss:0.072477\n",
      "[97]\teval-mlogloss:0.109713\ttrain-mlogloss:0.070856\n",
      "[98]\teval-mlogloss:0.107923\ttrain-mlogloss:0.069278\n",
      "[99]\teval-mlogloss:0.10639\ttrain-mlogloss:0.067941\n",
      "[100]\teval-mlogloss:0.104673\ttrain-mlogloss:0.066445\n",
      "[101]\teval-mlogloss:0.103108\ttrain-mlogloss:0.065115\n",
      "[102]\teval-mlogloss:0.101482\ttrain-mlogloss:0.063762\n",
      "[103]\teval-mlogloss:0.10001\ttrain-mlogloss:0.062513\n",
      "[104]\teval-mlogloss:0.098715\ttrain-mlogloss:0.061421\n",
      "[105]\teval-mlogloss:0.097057\ttrain-mlogloss:0.060052\n",
      "[106]\teval-mlogloss:0.095579\ttrain-mlogloss:0.05878\n",
      "[107]\teval-mlogloss:0.094344\ttrain-mlogloss:0.057746\n",
      "[108]\teval-mlogloss:0.093048\ttrain-mlogloss:0.056619\n",
      "[109]\teval-mlogloss:0.091837\ttrain-mlogloss:0.055635\n",
      "[110]\teval-mlogloss:0.090408\ttrain-mlogloss:0.05446\n",
      "\tScore 0.989833603054\n",
      "\n",
      "\n",
      "The best hyperparameters are:  \n",
      "\n",
      "{'colsample_bytree': 0.8500000000000001, 'min_child_weight': 4.0, 'n_estimators': 148.0, 'subsample': 0.9, 'eta': 0.15000000000000002, 'max_depth': 8, 'gamma': 0.75}\n"
     ]
    }
   ],
   "source": [
    "best_hyperparams = optimize()\n",
    "print(\"The best hyperparameters are: \", \"\\n\")\n",
    "print(best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train and test a classifier\n",
    "def train_and_test(X_tr, y_tr, X_v, well_v):\n",
    "    \n",
    "    # Feature normalization\n",
    "    scaler = preprocessing.RobustScaler(quantile_range=(25.0, 75.0)).fit(X_tr)\n",
    "    X_tr = scaler.transform(X_tr)\n",
    "    X_v = scaler.transform(X_v)\n",
    "    \n",
    "    # Train classifier\n",
    "    #clf = make_pipeline(make_union(VotingClassifier([(\"est\", ExtraTreesClassifier(criterion=\"gini\", max_features=1.0, n_estimators=500))]), FunctionTransformer(lambda X: X)), XGBClassifier(learning_rate=0.73, max_depth=10, min_child_weight=10, n_estimators=500, subsample=0.27))\n",
    "    #clf =  make_pipeline( KNeighborsClassifier(n_neighbors=5, weights=\"distance\") ) \n",
    "    #clf = make_pipeline(MaxAbsScaler(),make_union(VotingClassifier([(\"est\", RandomForestClassifier(n_estimators=500))]), FunctionTransformer(lambda X: X)),ExtraTreesClassifier(criterion=\"entropy\", max_features=0.0001, n_estimators=500))\n",
    "    # * clf = make_pipeline( make_union(VotingClassifier([(\"est\", BernoulliNB(alpha=60.0, binarize=0.26, fit_prior=True))]), FunctionTransformer(lambda X: X)),RandomForestClassifier(n_estimators=500))\n",
    "    # ** clf = make_pipeline ( XGBClassifier(learning_rate=0.12, max_depth=3, min_child_weight=10, n_estimators=150, seed = 17, colsample_bytree = 0.9) )\n",
    "    clf = clf = make_pipeline ( XGBClassifier(learning_rate=0.15, max_depth=8, min_child_weight=4, n_estimators=148, seed = SEED, colsample_bytree = 0.85, subsample = 0.9 , gamma = 0.75) )\n",
    "    clf.fit(X_tr, y_tr)\n",
    "    \n",
    "    # Test classifier\n",
    "    y_v_hat = clf.predict(X_v)\n",
    "    \n",
    "    # Clean isolated facies for each well\n",
    "    for w in np.unique(well_v):\n",
    "        y_v_hat[well_v==w] = medfilt(y_v_hat[well_v==w], kernel_size=5)\n",
    "    \n",
    "    return y_v_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#Load testing data\n",
    "test_data = pd.read_csv('../validation_data_nofacies.csv')\n",
    "\n",
    "# Prepare training data\n",
    "X_tr = X\n",
    "y_tr = y\n",
    "\n",
    "# Augment features\n",
    "X_tr, padded_rows = augment_features(X_tr, well, depth)\n",
    "\n",
    "# Removed padded rows\n",
    "X_tr = np.delete(X_tr, padded_rows, axis=0)\n",
    "y_tr = np.delete(y_tr, padded_rows, axis=0) - 1\n",
    "\n",
    "# Prepare test data\n",
    "well_ts = test_data['Well Name'].values\n",
    "depth_ts = test_data['Depth'].values\n",
    "X_ts = test_data[feature_names].values\n",
    "\n",
    "# Augment features\n",
    "X_ts, padded_rows = augment_features(X_ts, well_ts, depth_ts)\n",
    "\n",
    "# Predict test labels\n",
    "y_ts_hat = train_and_test(X_tr, y_tr, X_ts, well_ts)\n",
    "\n",
    "# Save predicted labels\n",
    "test_data['Facies'] = y_ts_hat + 1\n",
    "test_data.to_csv('Prediction_XXX_Final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
